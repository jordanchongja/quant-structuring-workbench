{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd64a23-0b0c-4150-b6f3-8081e552d910",
   "metadata": {},
   "source": [
    "WRDS Options Cleaner\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "1. **Audit:** Peek inside massive CSV files to see what dates/tickers they contain without loading the whole file.\n",
    "    \n",
    "2. **Filter:** Extract only the tickers we care about (e.g., `SPX` for S&P 500) to create a manageable dataset.\n",
    "    \n",
    "3. **Compress:** Save the cleaned data to `.parquet` format (much faster and smaller than CSV).\n",
    "    \n",
    "\n",
    "**Target Ticker:** `SPX` (Standard S&P 500 options) - Best for Heston calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373363-e4da-4c17-89da-bce231392de7",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95f01af5-d479-4180-a2e8-cb5881772658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè≠ Processing Option Prices: Options_Price_S&PDATAONLY_AllYears.gz...\n",
      "  > Scanned 2,500,000 rows...\n",
      "  > Scanned 5,000,000 rows...\n",
      "  > Scanned 7,500,000 rows...\n",
      "  > Scanned 10,000,000 rows...\n",
      "  > Scanned 12,500,000 rows...\n",
      "  > Scanned 15,000,000 rows...\n",
      "  > Scanned 17,500,000 rows...\n",
      "  > Scanned 20,000,000 rows...\n",
      "  > Scanned 22,500,000 rows...\n",
      "  > Scanned 25,000,000 rows...\n",
      "  > Scanned 27,500,000 rows...\n",
      "  > Scanned 30,000,000 rows...\n",
      "  > Scanned 32,500,000 rows...\n",
      "  > Scanned 35,000,000 rows...\n",
      "  > Scanned 37,500,000 rows...\n",
      "  > Scanned 40,000,000 rows...\n",
      "  > Scanned 42,500,000 rows...\n",
      "  > Scanned 45,000,000 rows...\n",
      "  > Scanned 47,500,000 rows...\n",
      "  > Scanned 50,000,000 rows...\n",
      "  > Scanned 52,500,000 rows...\n",
      "  > Scanned 55,000,000 rows...\n",
      "  > Scanned 57,500,000 rows...\n",
      "  > Scanned 60,000,000 rows...\n",
      "  > Scanned 62,500,000 rows...\n",
      "  > Scanned 65,000,000 rows...\n",
      "  > Scanned 67,500,000 rows...\n",
      "  > Scanned 70,000,000 rows...\n",
      "  > Scanned 72,500,000 rows...\n",
      "  > Scanned 75,000,000 rows...\n",
      "  > Scanned 77,500,000 rows...\n",
      "  > Scanned 80,000,000 rows...\n",
      "‚úÖ Success! Saved 48,335,273 rows to SPX_OptionPrices_Cleaned.parquet in 267.94s.\n",
      "\n",
      "üè≠ Processing Volatility Surface: Options_VolatilitySurface_S&PDATAONLY_AllYears.gz...\n",
      "  > Scanned 2,500,000 rows...\n",
      "  > Scanned 5,000,000 rows...\n",
      "‚úÖ Success! Saved 2,791,162 rows to SPX_VolSurface_Cleaned.parquet in 7.23s.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Set this to where your downloaded files are\n",
    "DATA_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\n",
    "\n",
    "# Put the exact names of your .gz files here:\n",
    "FILE_PRICES      = \"Options_Price_S&PDATAONLY_AllYears.gz\"       # UPDATE THIS\n",
    "FILE_VOL_SURFACE = \"Options_VolatilitySurface_S&PDATAONLY_AllYears.gz\"    # UPDATE THIS\n",
    "\n",
    "# Output names\n",
    "OUT_PRICES      = \"SPX_OptionPrices_Cleaned.parquet\"\n",
    "OUT_VOL_SURFACE = \"SPX_VolSurface_Cleaned.parquet\"\n",
    "\n",
    "TARGET_TICKER = \"SPX\"\n",
    "\n",
    "# --- 2. Processing Function ---\n",
    "def process_to_parquet(input_filename, output_filename, target_cols, file_type):\n",
    "    input_path = os.path.join(DATA_DIR, input_filename)\n",
    "    output_path = os.path.join(DATA_DIR, output_filename)\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå Skipping {file_type}: Could not find {input_filename}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"\\nüè≠ Processing {file_type}: {input_filename}...\")\n",
    "    start_time = time.time()\n",
    "    filtered_chunks = []\n",
    "    \n",
    "    try:\n",
    "        # Read in chunks, decompressing on the fly\n",
    "        chunk_iter = pd.read_csv(input_path, chunksize=500_000, compression='infer', low_memory=False)\n",
    "        \n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            chunk.columns = [c.lower() for c in chunk.columns]\n",
    "            \n",
    "            # Filter by ticker if the column exists\n",
    "            if 'ticker' in chunk.columns:\n",
    "                target_data = chunk[chunk['ticker'] == TARGET_TICKER].copy()\n",
    "            elif 'secid' in chunk.columns and 'ticker' not in chunk.columns:\n",
    "                # SPX secid is typically 108105 in OptionMetrics\n",
    "                target_data = chunk[chunk['secid'] == 108105].copy()\n",
    "            else:\n",
    "                target_data = chunk.copy() # Assume it's already filtered\n",
    "                \n",
    "            if not target_data.empty:\n",
    "                # Keep only available columns from our target list\n",
    "                available_cols = list(set(target_cols) & set(target_data.columns))\n",
    "                filtered_chunks.append(target_data[available_cols])\n",
    "                \n",
    "            if i % 5 == 0 and i > 0:\n",
    "                print(f\"  > Scanned {i * 500_000:,} rows...\")\n",
    "\n",
    "        if not filtered_chunks:\n",
    "            print(f\"‚ùå No SPX data found in {input_filename}!\")\n",
    "            return\n",
    "            \n",
    "        # Combine and Save\n",
    "        full_df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "        \n",
    "        # Convert dates\n",
    "        if 'date' in full_df.columns: full_df['date'] = pd.to_datetime(full_df['date'], errors='coerce')\n",
    "        if 'exdate' in full_df.columns: full_df['exdate'] = pd.to_datetime(full_df['exdate'], errors='coerce')\n",
    "        \n",
    "        full_df.to_parquet(output_path, index=False)\n",
    "        \n",
    "        elapsed = round(time.time() - start_time, 2)\n",
    "        print(f\"‚úÖ Success! Saved {len(full_df):,} rows to {output_filename} in {elapsed}s.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {input_filename}: {e}\")\n",
    "\n",
    "# --- 3. Execute ---\n",
    "\n",
    "# Columns for the Heston Playground\n",
    "price_cols = ['date', 'exdate', 'cp_flag', 'strike_price', 'best_bid', 'best_offer', 'impl_volatility', 'ticker']\n",
    "\n",
    "# Columns for Machine Learning\n",
    "surface_cols = ['date', 'days', 'delta', 'impl_volatility', 'ticker']\n",
    "\n",
    "# Run the Forge\n",
    "process_to_parquet(FILE_PRICES, OUT_PRICES, price_cols, \"Option Prices\")\n",
    "process_to_parquet(FILE_VOL_SURFACE, OUT_VOL_SURFACE, surface_cols, \"Volatility Surface\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6aaa9-4487-49a2-87f7-b3628d8ee588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aba4c-1f75-4292-ab0f-239ee69c533e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0d5a6-eec7-4486-a49e-4138725fdb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028070-95fb-404c-8c7a-85e9105ae5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc53d77-9cce-4262-adb3-bcf51c583680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-env)",
   "language": "python",
   "name": "fin-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
