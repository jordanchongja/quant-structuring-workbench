{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d373363-e4da-4c17-89da-bce231392de7",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f01af5-d479-4180-a2e8-cb5881772658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as si"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd64a23-0b0c-4150-b6f3-8081e552d910",
   "metadata": {},
   "source": [
    "## Convert WRDS data to .parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6aaa9-4487-49a2-87f7-b3628d8ee588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Configuration (Updated Path) ---\n",
    "BASE_DIR = r\"G:\\My Drive\\00) Interview Prep\\00) Quant\\Data Sources\\WRDS Data\\Returns\\Options\"\n",
    "\n",
    "# Input Files\n",
    "FILE_SPOT  = \"IndexPrices_alltime.gz\"\n",
    "FILE_YIELD = \"ZeroCouponYieldCurve.gz\"\n",
    "FILE_DIV   = \"IndexDividendYields.gz\"\n",
    "\n",
    "# Output Files\n",
    "OUT_SPOT  = \"SPX_IndexPrices.parquet\"\n",
    "OUT_YIELD = \"ZeroCouponYieldCurve.parquet\"\n",
    "OUT_DIV   = \"SPX_IndexDividendYields.parquet\"\n",
    "\n",
    "TARGET_TICKER = \"SPX\"\n",
    "TARGET_SECID = 108105 # OptionMetrics SPX SecID\n",
    "\n",
    "def convert_ancillary_file(input_file, output_file, filter_spx=False):\n",
    "    input_path = os.path.join(BASE_DIR, input_file)\n",
    "    output_path = os.path.join(BASE_DIR, output_file)\n",
    "    \n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"‚ùå Could not find {input_file}\")\n",
    "        return\n",
    "        \n",
    "    print(f\"üè≠ Converting {input_file}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # These are smaller than the options files, so we can usually read them directly, \n",
    "        # but chunking is safer for the 1.2GB IndexPrices file.\n",
    "        chunks = []\n",
    "        chunk_iter = pd.read_csv(input_path, chunksize=250_000, compression='gzip', low_memory=False)\n",
    "        \n",
    "        for chunk in chunk_iter:\n",
    "            chunk.columns = [c.lower() for c in chunk.columns]\n",
    "            \n",
    "            if filter_spx:\n",
    "                if 'ticker' in chunk.columns:\n",
    "                    target_data = chunk[chunk['ticker'] == TARGET_TICKER].copy()\n",
    "                elif 'secid' in chunk.columns:\n",
    "                    target_data = chunk[chunk['secid'] == TARGET_SECID].copy()\n",
    "                else:\n",
    "                    target_data = chunk.copy()\n",
    "            else:\n",
    "                target_data = chunk.copy()\n",
    "                \n",
    "            if not target_data.empty:\n",
    "                chunks.append(target_data)\n",
    "                \n",
    "        if not chunks:\n",
    "            print(f\"‚ùå No data extracted for {input_file}.\")\n",
    "            return\n",
    "            \n",
    "        full_df = pd.concat(chunks, ignore_index=True)\n",
    "        \n",
    "        # Standardize date columns\n",
    "        if 'date' in full_df.columns:\n",
    "            full_df['date'] = pd.to_datetime(full_df['date'], errors='coerce')\n",
    "            \n",
    "        full_df.to_parquet(output_path, index=False)\n",
    "        elapsed = round(time.time() - start_time, 2)\n",
    "        print(f\"‚úÖ Saved {len(full_df):,} rows to {output_file} in {elapsed}s.\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error processing {input_file}: {e}\\n\")\n",
    "\n",
    "# --- 2. Run the Conversions ---\n",
    "# We filter Spot and Div for SPX. Yield curve applies to everything, so no filter.\n",
    "convert_ancillary_file(FILE_SPOT, OUT_SPOT, filter_spx=True)\n",
    "convert_ancillary_file(FILE_YIELD, OUT_YIELD, filter_spx=False)\n",
    "convert_ancillary_file(FILE_DIV, OUT_DIV, filter_spx=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aba4c-1f75-4292-ab0f-239ee69c533e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0d5a6-eec7-4486-a49e-4138725fdb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028070-95fb-404c-8c7a-85e9105ae5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc53d77-9cce-4262-adb3-bcf51c583680",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-env)",
   "language": "python",
   "name": "fin-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
