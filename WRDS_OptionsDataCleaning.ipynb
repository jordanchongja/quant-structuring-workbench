{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cd64a23-0b0c-4150-b6f3-8081e552d910",
   "metadata": {},
   "source": [
    "WRDS Options Cleaner\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "1. **Audit:** Peek inside massive CSV files to see what dates/tickers they contain without loading the whole file.\n",
    "    \n",
    "2. **Filter:** Extract only the tickers we care about (e.g., `SPX` for S&P 500) to create a manageable dataset.\n",
    "    \n",
    "3. **Compress:** Save the cleaned data to `.parquet` format (much faster and smaller than CSV).\n",
    "    \n",
    "\n",
    "**Target Ticker:** `SPX` (Standard S&P 500 options) - Best for Heston calibration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d373363-e4da-4c17-89da-bce231392de7",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95f01af5-d479-4180-a2e8-cb5881772658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Update this path to where your screenshot folder is\n",
    "RAW_DATA_PATH = r\"G:\\My Drive\\00)Quant_Scripts\\WRDS Data\\Returns\\Options\" \n",
    "\n",
    "# The Ticker you want to extract for your Math Playground\n",
    "# Note: In OptionMetrics, S&P 500 is often 'SPX' or ticker_id '108105'\n",
    "TARGET_TICKER = 'SPX' \n",
    "\n",
    "# Output filename\n",
    "OUTPUT_FILE = \"SPX_Options_Cleaned_2025.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b10b3c5-b8f8-4f46-b471-31d232d8a8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning Directory...\n",
      "File Name                                | Size (GB)  | Columns detected\n",
      "----------------------------------------------------------------------------------------------------\n",
      "OptionVolume_1996-2025.csv               | 4.09       | secid, date, cp_flag, index_flag...\n",
      "OptionsVolatilitySurface_3.csv           | 42.55      | secid, date, days, delta...\n",
      "OptionsVolatilitySurface_4.csv           | 32.28      | secid, date, days, delta...\n",
      "OptionsVolatilitySurface_2.csv           | 38.05      | secid, date, days, delta...\n",
      "OptionsVolatilitySurface_1.csv           | 27.51      | secid, date, days, delta...\n",
      "OptionsVolatilitySurface_2024-2025.csv   | 41.92      | secid, date, days, delta...\n",
      "v1zilpeskdggmanu.csv                     | 43.13      | secid, date, days, delta...\n",
      "nbbtnjkbfwepkxdq.csv                     | 43.96      | secid, date, days, delta...\n",
      "OptionsVolatilitySurface1996-2025_Filtered.csv | 19.91      | secid, date, days, delta...\n"
     ]
    }
   ],
   "source": [
    "def audit_files(directory):\n",
    "    files = glob.glob(os.path.join(directory, \"*\")) # grab all files\n",
    "    \n",
    "    print(f\"{'File Name':<40} | {'Size (GB)':<10} | {'Columns detected'}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    valid_csvs = []\n",
    "    \n",
    "    for f in files:\n",
    "        if os.path.isdir(f): continue # skip folders\n",
    "        \n",
    "        # Get size\n",
    "        size_gb = os.path.getsize(f) / (1024**3)\n",
    "        \n",
    "        # Peek at content (try/except for non-csv garbage files)\n",
    "        try:\n",
    "            # Read only first 2 rows to check columns\n",
    "            df_peek = pd.read_csv(f, nrows=2)\n",
    "            cols = \", \".join(df_peek.columns[:4]) + \"...\" # Show first 4 cols\n",
    "            print(f\"{os.path.basename(f):<40} | {size_gb:<10.2f} | {cols}\")\n",
    "            valid_csvs.append(f)\n",
    "        except Exception as e:\n",
    "            print(f\"{os.path.basename(f):<40} | {size_gb:<10.2f} | [Error reading file]\")\n",
    "            \n",
    "    return valid_csvs\n",
    "\n",
    "# Run the Audit\n",
    "print(\"Scanning Directory...\")\n",
    "csv_files = audit_files(RAW_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5892e9-7729-4335-ba94-c83ed24bb568",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_filter(file_list, target_ticker):\n",
    "    filtered_chunks = []\n",
    "    total_rows_saved = 0\n",
    "    \n",
    "    # Columns we actually need (Change these based on what the Audit in Cell 3 shows!)\n",
    "    # Standard OptionMetrics names:\n",
    "    keep_cols = ['date', 'exdate', 'cp_flag', 'strike_price', 'best_bid', 'best_offer', 'impl_volatility', 'delta', 'gamma', 'vega', 'ticker']\n",
    "    \n",
    "    print(f\"\\nStarting Extraction for ticker: {target_ticker}\")\n",
    "    \n",
    "    for file_path in file_list:\n",
    "        print(f\"Processing {os.path.basename(file_path)}...\")\n",
    "        \n",
    "        # Create an iterator to read in chunks\n",
    "        # chunksize=1_000_000 means read 1 million rows at a time\n",
    "        chunk_iter = pd.read_csv(file_path, chunksize=1_000_000, low_memory=False)\n",
    "        \n",
    "        for i, chunk in enumerate(chunk_iter):\n",
    "            # 1. Standardize Column Names (Lowercase)\n",
    "            chunk.columns = [c.lower() for c in chunk.columns]\n",
    "            \n",
    "            # 2. Filter for Target\n",
    "            # Check if 'ticker' column exists, otherwise skip or guess\n",
    "            if 'ticker' in chunk.columns:\n",
    "                mask = chunk['ticker'] == target_ticker\n",
    "                target_data = chunk[mask].copy()\n",
    "                \n",
    "                if not target_data.empty:\n",
    "                    # 3. Keep only useful columns (intersection of what we want and what exists)\n",
    "                    available_cols = list(set(keep_cols) & set(target_data.columns))\n",
    "                    filtered_chunks.append(target_data[available_cols])\n",
    "                    total_rows_saved += len(target_data)\n",
    "                    \n",
    "            if i % 10 == 0:\n",
    "                print(f\"  > Scanned chunk {i}... Found {total_rows_saved} rows so far.\")\n",
    "\n",
    "    if not filtered_chunks:\n",
    "        print(\"No data found for this ticker!\")\n",
    "        return None\n",
    "        \n",
    "    print(\"Concatenating all chunks...\")\n",
    "    full_df = pd.concat(filtered_chunks, ignore_index=True)\n",
    "    \n",
    "    # Final Cleanup\n",
    "    # Convert dates to datetime objects\n",
    "    if 'date' in full_df.columns: pd.to_datetime(full_df['date'], format='%Y%m%d', errors='ignore')\n",
    "    if 'exdate' in full_df.columns: pd.to_datetime(full_df['exdate'], format='%Y%m%d', errors='ignore')\n",
    "    \n",
    "    # Sort\n",
    "    full_df = full_df.sort_values(by=['date', 'strike_price'])\n",
    "    \n",
    "    return full_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e586297-c5e7-4ec8-b89d-0ce4afc0ee27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_clean is not None:\n",
    "    print(f\"Saving {len(df_clean)} rows to {OUTPUT_FILE}...\")\n",
    "    \n",
    "    # Save to Parquet (requires pyarrow or fastparquet installed)\n",
    "    # pip install pyarrow\n",
    "    df_clean.to_parquet(OUTPUT_FILE, index=False)\n",
    "    \n",
    "    print(\"Success! Data Forge Complete.\")\n",
    "    \n",
    "    # Quick Preview\n",
    "    print(df_clean.head())\n",
    "else:\n",
    "    print(\"Failed to generate data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6aaa9-4487-49a2-87f7-b3628d8ee588",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87aba4c-1f75-4292-ab0f-239ee69c533e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e0d5a6-eec7-4486-a49e-4138725fdb38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58b0cba2-7671-4271-9137-dd2bf7ce6670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                     | Start Date   | End Date    \n",
      "---------------------------------------------------------------------------\n",
      "OptionsVolatilitySurface_3.csv                | 2024-01-02   | 2024-12-31  \n",
      "OptionsVolatilitySurface_4.csv                | 2020-01-02   | 2020-12-31  \n",
      "OptionsVolatilitySurface_2.csv                | 2021-01-04   | 2021-12-31  \n",
      "OptionsVolatilitySurface_1.csv                | 2025-01-02   | 2025-08-29  \n",
      "OptionsVolatilitySurface_2024-2025.csv        | 2024-08-29   | 2025-08-29  \n",
      "v1zilpeskdggmanu.csv                          | 2023-08-29   | 2024-08-29  \n",
      "nbbtnjkbfwepkxdq.csv                          | 2022-08-29   | 2023-08-29  \n",
      "OptionsVolatilitySurface1996-2025_Filtered.csv | 1996-01-04   | 2025-08-29  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def get_csv_date_range(file_path, date_col_name='date'):\n",
    "    \"\"\"\n",
    "    Efficiently reads the first and last recorded date in a large CSV\n",
    "    without loading the entire file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Get the Header and First Date (Start)\n",
    "        # Read just the first 5 rows\n",
    "        df_head = pd.read_csv(file_path, nrows=5)\n",
    "        \n",
    "        # Normalize column names to lower case to find 'date'\n",
    "        df_head.columns = [c.lower() for c in df_head.columns]\n",
    "        \n",
    "        if date_col_name not in df_head.columns:\n",
    "            return \"Col Not Found\", \"Col Not Found\"\n",
    "            \n",
    "        start_date = df_head[date_col_name].iloc[0]\n",
    "        \n",
    "        # 2. Get the Last Date (End) using file seeking\n",
    "        # This jumps to the end of the file byte-by-byte to read the tail\n",
    "        with open(file_path, 'rb') as f:\n",
    "            f.seek(0, os.SEEK_END) # Jump to end\n",
    "            file_size = f.tell()\n",
    "            \n",
    "            # Seek back ~4096 bytes (should be enough for last few rows)\n",
    "            seek_offset = min(file_size, 4096) \n",
    "            f.seek(-seek_offset, os.SEEK_END)\n",
    "            \n",
    "            # Read tail and decode\n",
    "            tail_lines = f.read().decode('utf-8', errors='ignore').splitlines()\n",
    "            \n",
    "            # The last line might be empty or incomplete, grab the last valid one\n",
    "            last_line = tail_lines[-1] if tail_lines[-1] else tail_lines[-2]\n",
    "            \n",
    "            # Parse the CSV string manually\n",
    "            reader = csv.reader([last_line])\n",
    "            last_row = list(reader)[0]\n",
    "            \n",
    "            # We need to map the 'date' column index from the header\n",
    "            col_idx = df_head.columns.get_loc(date_col_name)\n",
    "            end_date = last_row[col_idx]\n",
    "            \n",
    "        return start_date, end_date\n",
    "\n",
    "    except Exception as e:\n",
    "        return \"Error\", str(e)\n",
    "\n",
    "# --- RUN THE DATE SCOUT ---\n",
    "data_dir = r\"G:\\My Drive\\00)Quant_Scripts\\WRDS Data\\Returns\\Options\" # Update if needed\n",
    "files = [f for f in os.listdir(data_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "print(f\"{'File Name':<45} | {'Start Date':<12} | {'End Date':<12}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for f in files:\n",
    "    f_path = os.path.join(data_dir, f)\n",
    "    # Skip the small volume file or processed files if you want\n",
    "    if \"OptionVolume\" in f: continue \n",
    "    \n",
    "    start, end = get_csv_date_range(f_path)\n",
    "    print(f\"{f:<45} | {str(start):<12} | {str(end):<12}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "911308cc-bd4d-4dee-a28f-295499b6f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Inspecting the Golden Ticket: OptionsVolatilitySurface1996-2025_Filtered.csv...\n",
      "\n",
      "Starting Extraction for ticker: SPX\n",
      "Processing OptionsVolatilitySurface1996-2025_Filtered.csv...\n",
      "  > Scanned chunk 0... Found 0 rows so far.\n",
      "  > Scanned chunk 10... Found 4545 rows so far.\n",
      "  > Scanned chunk 20... Found 9081 rows so far.\n",
      "  > Scanned chunk 30... Found 13581 rows so far.\n",
      "  > Scanned chunk 40... Found 18117 rows so far.\n",
      "  > Scanned chunk 50... Found 20385 rows so far.\n",
      "  > Scanned chunk 60... Found 24912 rows so far.\n",
      "  > Scanned chunk 70... Found 29448 rows so far.\n",
      "  > Scanned chunk 80... Found 31716 rows so far.\n",
      "  > Scanned chunk 90... Found 33984 rows so far.\n",
      "  > Scanned chunk 100... Found 36252 rows so far.\n",
      "  > Scanned chunk 110... Found 40770 rows so far.\n",
      "  > Scanned chunk 120... Found 43038 rows so far.\n",
      "  > Scanned chunk 130... Found 45306 rows so far.\n",
      "  > Scanned chunk 140... Found 47574 rows so far.\n",
      "  > Scanned chunk 150... Found 49833 rows so far.\n",
      "  > Scanned chunk 160... Found 52092 rows so far.\n",
      "  > Scanned chunk 170... Found 54360 rows so far.\n",
      "  > Scanned chunk 180... Found 56637 rows so far.\n",
      "  > Scanned chunk 190... Found 58905 rows so far.\n",
      "  > Scanned chunk 200... Found 61164 rows so far.\n",
      "  > Scanned chunk 210... Found 63414 rows so far.\n",
      "  > Scanned chunk 220... Found 63414 rows so far.\n",
      "  > Scanned chunk 230... Found 65682 rows so far.\n",
      "  > Scanned chunk 240... Found 67167 rows so far.\n",
      "Concatenating all chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_58852\\79346750.py:46: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_datetime without passing `errors` and catch exceptions explicitly instead\n",
      "  if 'date' in full_df.columns: pd.to_datetime(full_df['date'], format='%Y%m%d', errors='ignore')\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'strike_price'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_58852\\133375582.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# 2. Run the Refinery on JUST this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# We reuse the logic but strictly for one file to save time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;31m# Process\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mdf_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_and_filter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_ticker\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SPX'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdf_clean\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdf_clean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"‚úÖ Success! Found {len(df_clean):,} rows for SPX.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_58852\\79346750.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(file_list, target_ticker)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'date'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y%m%d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m'exdate'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'exdate'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'%Y%m%d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[1;31m# Sort\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m     \u001b[0mfull_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'strike_price'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mfull_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7190\u001b[0m                 \u001b[1;34mf\"Length of ascending ({len(ascending)})\"\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m                 \u001b[1;34mf\" != length of by ({len(by)})\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             )\n\u001b[0;32m   7193\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7194\u001b[1;33m             \u001b[0mkeys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mby\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7196\u001b[0m             \u001b[1;31m# need to rewrap columns in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7197\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m-> 7194\u001b[1;33m         \u001b[1;33m...\u001b[0m     \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_natsorted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"time\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\anaconda3\\envs\\fin-env\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1910\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1911\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1912\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1914\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1915\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1916\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'strike_price'"
     ]
    }
   ],
   "source": [
    "# Save this in a new cell in your Data Forge notebook\n",
    "\n",
    "# 1. Setup - Only target the big file\n",
    "target_file = \"OptionsVolatilitySurface1996-2025_Filtered.csv\"\n",
    "file_path = os.path.join(RAW_DATA_PATH, target_file)\n",
    "output_parquet = \"SPX_Master_1996_2025.parquet\"\n",
    "\n",
    "print(f\"üïµÔ∏è Inspecting the Golden Ticket: {target_file}...\")\n",
    "\n",
    "# 2. Run the Refinery on JUST this file\n",
    "# We reuse the logic but strictly for one file to save time\n",
    "if os.path.exists(file_path):\n",
    "    # Process\n",
    "    df_clean = process_and_filter([file_path], target_ticker='SPX')\n",
    "    \n",
    "    if df_clean is not None and not df_clean.empty:\n",
    "        print(f\"‚úÖ Success! Found {len(df_clean):,} rows for SPX.\")\n",
    "        print(f\"Dates covered: {df_clean['date'].min()} to {df_clean['date'].max()}\")\n",
    "        \n",
    "        # Save\n",
    "        df_clean.to_parquet(output_parquet, index=False)\n",
    "        print(f\"üíæ Saved to {output_parquet}. You can now delete the CSVs!\")\n",
    "    else:\n",
    "        print(\"‚ùå Bad News: The filtered file exists but had no 'SPX' data.\")\n",
    "        print(\"You might need to download the missing 'Jan 2022 - Aug 2022' block from WRDS.\")\n",
    "else:\n",
    "    print(f\"‚ùå File not found: {target_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a94015f-f912-4e26-8b32-fcd01301eea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üïµÔ∏è Inspecting columns for: Options_Raw_2024_FullYear.csv\n",
      "\n",
      "--- COLUMNS FOUND ---\n",
      "- secid\n",
      "- date\n",
      "- days\n",
      "- delta\n",
      "- impl_volatility\n",
      "- impl_strike\n",
      "- impl_premium\n",
      "- dispersion\n",
      "- cp_flag\n",
      "- cusip\n",
      "- ticker\n",
      "- sic\n",
      "- index_flag\n",
      "- exchange_d\n",
      "- class\n",
      "- issue_type\n",
      "- industry_group\n",
      "\n",
      "--- DIAGNOSIS ---\n",
      "‚ùå BAD NEWS: No strike column found. This is likely the Volatility Surface dataset.\n",
      "üëâ ACTION: You will need to download 'Option Prices' from WRDS.\n"
     ]
    }
   ],
   "source": [
    "# Save this in a new cell in your Data Forge notebook\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Point this to your 2024 file (which we renamed earlier)\n",
    "# If you haven't renamed it yet, use the original name: \"OptionsVolatilitySurface_3.csv\"\n",
    "file_path = os.path.join(r\"G:\\My Drive\\00)Quant_Scripts\\WRDS Data\\Returns\\Options\", \"Options_Raw_2024_FullYear.csv\")\n",
    "\n",
    "# Fallback if you didn't run the rename script yet\n",
    "if not os.path.exists(file_path):\n",
    "    file_path = os.path.join(r\"G:\\My Drive\\00)Quant_Scripts\\WRDS Data\\Returns\\Options\", \"OptionsVolatilitySurface_3.csv\")\n",
    "\n",
    "print(f\"üïµÔ∏è Inspecting columns for: {os.path.basename(file_path)}\")\n",
    "\n",
    "try:\n",
    "    # Read just the header (0 rows)\n",
    "    df_head = pd.read_csv(file_path, nrows=0)\n",
    "    \n",
    "    print(\"\\n--- COLUMNS FOUND ---\")\n",
    "    for col in df_head.columns:\n",
    "        print(f\"- {col}\")\n",
    "        \n",
    "    print(\"\\n--- DIAGNOSIS ---\")\n",
    "    if 'strike_price' in [c.lower() for c in df_head.columns]:\n",
    "        print(\"‚úÖ GOOD NEWS: 'strike_price' exists! You can use this file.\")\n",
    "    elif 'strike' in [c.lower() for c in df_head.columns]:\n",
    "        print(\"‚úÖ GOOD NEWS: Found 'strike' column (just need to rename it).\")\n",
    "    else:\n",
    "        print(\"‚ùå BAD NEWS: No strike column found. This is likely the Volatility Surface dataset.\")\n",
    "        print(\"üëâ ACTION: You will need to download 'Option Prices' from WRDS.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96028070-95fb-404c-8c7a-85e9105ae5b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (fin-env)",
   "language": "python",
   "name": "fin-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
